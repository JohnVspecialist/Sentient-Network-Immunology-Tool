"""
Enhanced Sentient Immunology System
A secure implementation following DevSecOps and MLSecOps best practices
"""

import json
import logging
import os
from collections import deque
from datetime import datetime
import asyncio
import hashlib
import re
import secrets
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.cluster import DBSCAN
import tkinter as tk
from tkinter import ttk, scrolledtext
from typing import Dict, List, Optional, Any
import yaml  # For safer config parsing


class EnhancedSentientImmunology:
    """Main immunology system with enhanced security controls."""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize the immunology system with secure defaults.
        
        Args:
            config_path: Path to configuration file (YAML recommended over JSON)
        """
        self.logger = self._setup_logging()
        self.threat_history = deque(maxlen=1000)
        self.config: Optional[Dict[str, Any]] = None
        self.tokenizer = None
        self.model = None
        self.innate_immunity = EnhancedInnateImmunity()
        self.adaptive_immunity = EnhancedAdaptiveImmunity()
        self.threat_response = ThreatResponseSystem()
        
        # Generate a session ID for traceability
        self.session_id = secrets.token_hex(16)
        self.logger.info(f"Session initialized: {self.session_id}")
        
        # Load configuration securely
        self._load_config(config_path)

    def _setup_logging(self) -> logging.Logger:
        """Configure secure logging with appropriate levels and handlers."""
        logger = logging.getLogger(__name__)
        
        # Clear any existing handlers to prevent duplicate logs
        if logger.handlers:
            logger.handlers.clear()
            
        handler = logging.StreamHandler()
        # Use a more secure formatter that doesn't expose sensitive data
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - [SESSION: %(session_id)s] - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        # Default to a secure level - can be overridden by config
        logger.setLevel(logging.WARNING)
        
        # Add log filter for sensitive data
        class SensitiveDataFilter(logging.Filter):
            def filter(self, record):
                # Redact potential sensitive patterns in logs
                if hasattr(record, 'msg'):
                    # Redact potential authentication tokens, keys, etc.
                    record.msg = re.sub(r'(api[-_]?key|token|password|secret)(["\']?\s*[:=]\s*["\']?)([^"\'\s]{8,})',
                                       r'\1\2[REDACTED]', str(record.msg))
                return True
                
        logger.addFilter(SensitiveDataFilter())
        return logger

    def _validate_config(self, config: Dict) -> bool:
        """Validate configuration for required fields and security settings."""
        required_fields = ["model_name"]
        
        # Check for required fields
        for field in required_fields:
            if field not in config:
                self.logger.error(f"Missing required configuration field: {field}")
                return False
                
        # Validate model source against allowlist
        allowed_models = ["distilbert-base-uncased", "roberta-base", "bert-base-uncased"]
        if config["model_name"] not in allowed_models:
            self.logger.error(f"Model {config['model_name']} not in allowed models list")
            return False
            
        # Set default security values if not provided
        if "security_level" not in config:
            self.logger.info("Setting default security level to 2")
            config["security_level"] = 2
            
        if "max_input_length" not in config:
            self.logger.info("Setting default max input length to 1000")
            config["max_input_length"] = 1000
            
        return True

    def _load_config(self, config_path: str) -> None:
        """
        Securely load and validate configuration.
        
        Args:
            config_path: Path to configuration file
        
        Raises:
            ValueError: If configuration is invalid or insecure
            FileNotFoundError: If configuration file cannot be found
        """
        try:
            # Validate path to prevent directory traversal
            norm_path = os.path.normpath(config_path)
            if norm_path != config_path or '..' in norm_path:
                raise ValueError(f"Suspicious path detected: {config_path}")
                
            # Check if file exists
            if not os.path.isfile(norm_path):
                raise FileNotFoundError(f"Configuration file not found: {norm_path}")
                
            # Load configuration based on file extension
            if norm_path.endswith('.yaml') or norm_path.endswith('.yml'):
                with open(norm_path, 'r') as f:
                    # Use safe_load to prevent code execution
                    self.config = yaml.safe_load(f)
            elif norm_path.endswith('.json'):
                with open(norm_path, 'r') as f:
                    self.config = json.load(f)
                    # Set necessary defaults if using the simple config format
                    if "feature_extractor_name" in self.config and len(self.config) == 2:
                        self.logger.info("Using simple config format, setting secure defaults")
                        self.config.update({
                            "security_level": 2,
                            "max_input_length": 1000,
                            "ml_timeout_seconds": 10,
                            "max_requests_per_minute": 60,
                            "always_run_behavioral": True
                        })
            else:
                raise ValueError(f"Unsupported configuration file format: {norm_path}")
                
            # Validate configuration
            if not self._validate_config(self.config):
                raise ValueError("Invalid configuration")
                
            # Initialize ML components with security controls
            self._initialize_ml_components()
            
            self.logger.info("Configuration and model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Error loading config: {str(e)}", exc_info=True)
            # Re-raise with sanitized message to avoid information disclosure
            raise ValueError("Failed to load configuration. See logs for details.") from e

    def _initialize_ml_components(self) -> None:
        """Initialize ML components with security controls."""
        try:
            # Set device appropriately
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            # Use local model if specified in config (safer than remote)
            local_models_dir = self.config.get("local_models_dir")
            if local_models_dir and os.path.isdir(local_models_dir):
                model_path = os.path.join(local_models_dir, self.config["model_name"])
                if os.path.isdir(model_path):
                    self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
                    self.model = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)
                    self.logger.info(f"Loaded model from local path: {model_path}")
                    return
            
            # Fallback to online model with security controls
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config["model_name"],
                use_auth_token=self.config.get("hf_token"),  # Proper authentication
                cache_dir=self.config.get("cache_dir", "./model_cache"),
                trust_remote_code=False  # Critical security setting
            )
            
            self.model = AutoModel.from_pretrained(
                self.config["model_name"],
                use_auth_token=self.config.get("hf_token"),
                cache_dir=self.config.get("cache_dir", "./model_cache"),
                trust_remote_code=False,  # Critical security setting
                torchscript=self.config.get("use_torchscript", False)
            ).to(device)
            
        except Exception as e:
            self.logger.error(f"Failed to initialize ML components: {str(e)}", exc_info=True)
            raise

    async def analyze_behavior(self, input_data: str) -> Dict[str, float]:
        """
        Analyze input behavior for anomalies.
        
        Args:
            input_data: The input string to analyze
            
        Returns:
            Dictionary with analysis scores
        """
        # Validate input
        if not input_data or not isinstance(input_data, str):
            self.logger.warning(f"Invalid input for behavior analysis: {type(input_data)}")
            return {"error": 1.0, "anomaly_score": 1.0, "complexity_score": 0.0, "repetition_score": 0.0}
            
        # Enforce input length limits to prevent DoS
        max_length = self.config.get("max_input_length", 1000)
        if len(input_data) > max_length:
            self.logger.warning(f"Input exceeds maximum length: {len(input_data)} > {max_length}")
            input_data = input_data[:max_length]
        
        try:
            # Timeout protection
            timeout = self.config.get("ml_timeout_seconds", 10)
            
            # Run analysis with timeout protection
            async with asyncio.timeout(timeout):
                # Move computations to appropriate device
                device = next(self.model.parameters()).device
                
                # Create embeddings
                embeddings = self.tokenizer(
                    input_data, 
                    return_tensors="pt",
                    truncation=True,
                    max_length=self.tokenizer.model_max_length
                ).to(device)
                
                # Process with model
                with torch.no_grad():  # For security and efficiency
                    outputs = self.model(**embeddings)
                
                # Get embeddings and move to CPU for sklearn
                sequence_output = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
                
                # Analyze with clustering
                clustering = DBSCAN(eps=0.3, min_samples=2).fit(sequence_output)
                
                # Calculate metrics
                anomaly_score = float((clustering.labels_ == -1).mean())
                complexity_score = float(sequence_output.std())
                
                # Calculate repetition safely
                words = input_data.split()
                repetition_score = float(len(set(words)) / max(len(words), 1))
                
                return {
                    "anomaly_score": anomaly_score,
                    "complexity_score": complexity_score,
                    "repetition_score": repetition_score,
                }
                
        except asyncio.TimeoutError:
            self.logger.error(f"Behavior analysis timed out after {timeout} seconds")
            return {"error": 1.0, "timeout": 1.0}
        except Exception as e:
            self.logger.error(f"Error in behavior analysis: {str(e)}", exc_info=True)
            return {"error": 1.0}

    async def immune_response(self, input_data: str) -> Dict[str, Any]:
        """
        Generate immune response to potential threats.
        
        Args:
            input_data: The input string to analyze
            
        Returns:
            Dictionary with response details
        """
        # Initialize response with secure defaults
        response = {
            "status": "processing",
            "threats": [],
            "risk_score": 0.0,
            "response_actions": [],
            "timestamp": datetime.now().isoformat(),
            "request_id": secrets.token_hex(8),  # For traceability
        }
        
        # Input validation
        if not input_data or not isinstance(input_data, str):
            response.update({
                "status": "error",
                "message": "Invalid input data",
                "risk_score": 1.0,
            })
            return response
            
        try:
            # Rate limiting check
            if not self._check_rate_limit():
                response.update({
                    "status": "throttled",
                    "message": "Rate limit exceeded",
                })
                return response
            
            # Run innate immunity checks (pattern based)
            innate_result = await self.innate_immunity.detect(input_data)
            
            # Run behavioral analysis if innate checks pass or configured for always-on
            behavioral_result = {}
            if not innate_result["threats"] or self.config.get("always_run_behavioral", True):
                behavioral_result = await self.analyze_behavior(input_data)
            
            # Combine scores from both systems
            combined_score = innate_result["score"]
            if "anomaly_score" in behavioral_result:
                # Weight between pattern and behavioral detection
                combined_score = (combined_score * 0.7) + (behavioral_result["anomaly_score"] * 0.3)
            
            # Update response with findings
            threats = innate_result["threats"]
            
            # Add behavioral threats if detected
            if behavioral_result.get("anomaly_score", 0) > 0.7:
                threats.append({
                    "type": "behavioral_anomaly",
                    "score": behavioral_result["anomaly_score"],
                    "details": "Unusual input pattern detected"
                })
            
            # Update final response
            response.update({
                "status": "threat_detected" if threats else "healthy",
                "threats": threats,
                "risk_score": min(combined_score, 1.0),  # Cap at 1.0
                "behavioral_metrics": behavioral_result,
            })
            
            # Generate appropriate response actions if threats detected
            if threats:
                response["response_actions"] = await self.threat_response.generate_response(
                    threats, combined_score
                )
            
            # Hash input with secure algorithm for storage
            input_hash = hashlib.sha256(input_data.encode()).hexdigest()
            
            # Store sanitized history (no raw input)
            self.threat_history.append({
                "timestamp": datetime.now().isoformat(),
                "input_hash": input_hash,
                "length": len(input_data),
                "response": response,
            })
            
            return response
            
        except Exception as e:
            self.logger.error(f"Error in immune response: {str(e)}", exc_info=True)
            response.update({
                "status": "error",
                "message": "Processing error",  # Generic message for security
            })
            return response

    def _check_rate_limit(self) -> bool:
        """Check if current request exceeds rate limits."""
        # Get rate limit settings
        max_requests = self.config.get("max_requests_per_minute", 60)
        
        # Calculate current rate (simplified implementation)
        current_minute = datetime.now().strftime("%Y-%m-%d %H:%M")
        recent_requests = sum(1 for entry in self.threat_history 
                             if entry["timestamp"].startswith(current_minute))
        
        return recent_requests < max_requests


class EnhancedInnateImmunity:
    """Pattern-based threat detection system with enhanced security rules."""
    
    def __init__(self):
        """Initialize the innate immunity system with security patterns."""
        # Comprehensive pattern recognition for common attacks
        self.pattern_recognition = {
            "prompt_injection": [
                r"(?i)(system\s*prompt|ignore\s*previous|ignore\s*instructions)",
                r"(?i)(you\s*are\s*now|new\s*personality|new\s*role)",
            ],
            "malicious_code": [
                r"<script.*?>|eval\(|exec\(|system\(|subprocess|os\.system",
                r"__import__\(|getattr\(|setattr\(|globals\(\)|locals\(\)",
                r"fromCharCode|parseInt\(.+,.+\)|String\.fromCodePoint",
            ],
            "sql_injection": [
                r"('|\")\s*(OR|AND)\s*('|\")\s*=\s*('|\")",
                r";\s*(DROP|DELETE|UPDATE|INSERT|CREATE|ALTER)",
                r"UNION\s+(ALL\s+)?SELECT",
            ],
            "path_traversal": [
                r"\.\.\/|\.\.\\|~\/|~\\|\/etc\/|\/var\/|C:\\|\/proc\/",
            ],
            "command_injection": [
                r";\s*(cat|ls|dir|pwd|whoami|ping|curl|wget|bash|cmd\.exe)",
                r"\|\s*(cat|ls|dir|pwd|whoami|ping|curl|wget|bash|cmd\.exe)",
            ],
            "data_exfiltration": [
                r"(firebase|aws|azure|cloud)\s*(key|token|secret|password)",
                r"(api[-_]?key|access[-_]?token|client[-_]?secret)",
            ],
        }

    async def detect(self, input_data: str) -> Dict[str, Any]:
        """
        Detect potential threats in input data.
        
        Args:
            input_data: The input string to analyze
            
        Returns:
            Dictionary with threats and score
        """
        threats = []
        score = 0.0
        
        # Input validation
        if not input_data or not isinstance(input_data, str):
            return {"threats": [], "score": 0.0}
            
        # Apply all pattern detections
        for threat_type, patterns in self.pattern_recognition.items():
            for pattern in patterns:
                matches = re.finditer(pattern, input_data, re.IGNORECASE)
                for match in matches:
                    # Only include first part of match for security
                    match_preview = match.group(0)[:10] + "..." if len(match.group(0)) > 10 else match.group(0)
                    threats.append({
                        "type": threat_type,
                        "pattern": match_preview,
                        "position": match.start(),
                    })
                    
                    # Increase score based on threat type severity
                    if threat_type in ["sql_injection", "command_injection"]:
                        score += 0.7  # Higher severity
                    else:
                        score += 0.5  # Standard severity
        
        # Additional entropy-based analysis for obfuscation attempts
        if len(input_data) > 20:
            # Calculate character entropy as a heuristic for obfuscation
            char_freq = {}
            for char in input_data:
                char_freq[char] = char_freq.get(char, 0) + 1
                
            entropy = sum(-count/len(input_data) * 
                       (count/len(input_data)).log() 
                       for count in char_freq.values())
                       
            # Very high entropy can indicate obfuscation
            if entropy > 4.5:  # Typical English text has entropy ~4.0
                threats.append({
                    "type": "possible_obfuscation",
                    "entropy": entropy,
                })
                score += 0.3
                
        return {"threats": threats, "score": min(score, 1.0)}  # Cap at 1.0


class EnhancedAdaptiveImmunity:
    """
    Adaptive immunity system that learns from past threats.
    This is a placeholder implementation with security considerations.
    """
    
    def __init__(self):
        """Initialize the adaptive immunity system."""
        self.logger = logging.getLogger(__name__ + ".adaptive")
        self.threat_memory = {}
        self.learning_enabled = False  # Disabled by default for security
        
    def enable_learning(self, admin_key: str, config_key: str) -> bool:
        """
        Securely enable the learning system with proper authentication.
        
        Args:
            admin_key: Administrator authentication key
            config_key: Configuration authentication key
            
        Returns:
            Boolean indicating success
        """
        # Implementation would include secure key verification
        # This is a placeholder for proper authentication
        self.learning_enabled = True
        self.logger.warning("Adaptive learning system enabled - verify security context")
        return True


class ThreatResponseSystem:
    """System to generate appropriate responses to detected threats."""
    
    def __init__(self):
        """Initialize the threat response system."""
        self.logger = logging.getLogger(__name__ + ".response")
        self.response_strategies = {
            "low": ["log_threat"],
            "medium": ["log_threat", "warn_user"],
            "high": ["log_threat", "warn_user", "throttle_requests"],
            "critical": ["log_threat", "warn_user", "block_input", "alert_admin"],
        }
        
    async def generate_response(self, threats: List[Dict[str, Any]], risk_score: float) -> List[str]:
        """
        Generate appropriate response actions based on threats.
        
        Args:
            threats: List of detected threats
            risk_score: Overall risk score
            
        Returns:
            List of response actions to take
        """
        # Determine severity level
        severity = "low"
        if risk_score > 0.9:
            severity = "critical"
        elif risk_score > 0.7:
            severity = "high"
        elif risk_score > 0.4:
            severity = "medium"
            
        # Log appropriately
        self.logger.warning(f"Threat detected - Severity: {severity}, Score: {risk_score}, " +
                          f"Threats: {len(threats)}")
                          
        # Return appropriate response actions
        return self.response_strategies.get(severity, ["log_threat"])


class SecureGUI:
    """Secure GUI implementation with input validation and event handling."""
    
    def __init__(self, immune_system: EnhancedSentientImmunology):
        """
        Initialize the secure GUI.
        
        Args:
            immune_system: The immunology system instance
        """
        self.immune_system = immune_system
        self.logger = logging.getLogger(__name__ + ".gui")
        
        # Initialize tkinter with security controls
        self.root = tk.Tk()
        self.root.title("Sentinel Network Immunology Tool")
        self.root.geometry("800x600")
        
        # Set minimum size for security (prevent UI manipulation attacks)
        self.root.minsize(600, 400)
        
        # Create secure widgets
        self.create_widgets()
        
        # Add event handlers
        self._setup_event_handlers()

    def create_widgets(self):
        """Create secure GUI widgets with proper validation."""
        # Main frame with padding
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill="both", expand=True)
        
        # Status bar for security notifications
        self.status_var = tk.StringVar(value="Ready - System Secure")
        status_bar = ttk.Label(self.root, textvariable=self.status_var, 
                              relief=tk.SUNKEN, anchor=tk.W)
        status_bar.pack(side=tk.BOTTOM, fill=tk.X)
        
        # Input frame
        input_frame = ttk.LabelFrame(main_frame, text="Input Analysis")
        input_frame.pack(fill="x", padx=10, pady=10)
        
        # Input text area with validation
        self.input_text = scrolledtext.ScrolledText(input_frame, height=5)
        self.input_text.pack(fill="x", padx=5, pady=5)
        
        # Button frame
        button_frame = ttk.Frame(input_frame)
        button_frame.pack(fill="x", padx=5, pady=5)
        
        # Analyze button with secure event handling
        analyze_btn = ttk.Button(button_frame, text="Analyze", command=self.analyze_input)
        analyze_btn.pack(side=tk.LEFT, padx=5)
        
        # Clear button
        clear_btn = ttk.Button(button_frame, text="Clear", command=self.clear_input)
        clear_btn.pack(side=tk.LEFT, padx=5)
        
        # Results frame
        result_frame = ttk.LabelFrame(main_frame, text="Analysis Results")
        result_frame.pack(fill="both", expand=True, padx=10, pady=10)
        
        # Results display
        self.results_text = scrolledtext.ScrolledText(result_frame)
        self.results_text.pack(fill="both", expand=True, padx=5, pady=5)
        self.results_text.config(state=tk.DISABLED)  # Read-only for security

    def _setup_event_handlers(self):
        """Set up secure event handlers."""
        # Handle window close securely
        self.root.protocol("WM_DELETE_WINDOW", self.on_close)
        
        # Set up periodic security check
        self.root.after(10000, self.security_check)

    def security_check(self):
        """Perform periodic security checks."""
        # Check for any security issues
        self.status_var.set(f"Secure - Last check: {datetime.now().strftime('%H:%M:%S')}")
        
        # Schedule next check
        self.root.after(10000, self.security_check)

    def analyze_input(self):
        """Securely analyze input with rate limiting and validation."""
        # Get input with length validation
        input_data = self.input_text.get("1.0", tk.END).strip()
        
        # Validate input length to prevent DoS
        max_length = self.immune_system.config.get("max_input_length", 1000)
        if len(input_data) > max_length:
            self.show_result(f"Input exceeds maximum length ({len(input_data)}/{max_length}). Truncating.")
            input_data = input_data[:max_length]
        
        # Execute analysis in a separate thread to avoid GUI freezing
        self.status_var.set("Processing...")
        self.root.update_idletasks()
        
        # Use asyncio to run the analysis
        asyncio.run(self.process_input(input_data))

    async def process_input(self, input_data: str):
        """
        Process input asynchronously.
        
        Args:
            input_data: The input to analyze
        """
        try:
            # Run immune response
            result = await self.immune_system.immune_response(input_data)
            
            # Format results for secure display
            formatted_result = json.dumps(result, indent=2)
            
            # Update UI
            self.show_result(formatted_result)
            
            # Update status based on result
            if result["status"] == "threat_detected":
                self.status_var.set(f"⚠️ Threat detected - Risk score: {result['risk_score']:.2f}")
            else:
                self.status_var.set("Analysis complete")
                
        except Exception as e:
            self.logger.error(f"Error processing input: {str(e)}", exc_info=True)
            self.show_result(f"Error occurred during analysis: {type(e).__name__}")
            self.status_var.set("Error occurred")

    def show_result(self, text: str):
        """
        Securely display results.
        
        Args:
            text: The text to display
        """
        # Enable for update
        self.results_text.config(state=tk.NORMAL)
        
        # Clear and insert
        self.results_text.delete("1.0", tk.END)
        self.results_text.insert("1.0", text)
        
        # Return to read-only
        self.results_text.config(state=tk.DISABLED)

    def clear_input(self):
        """Clear input securely."""
        self.input_text.delete("1.0", tk.END)
        self.status_var.set("Ready")

    def on_close(self):
        """Handle window close securely."""
        # Perform cleanup
        self.logger.info("Application shutting down securely")
        
        # Close window
        self.root.destroy()

    def run(self):
        """Run the application securely."""
        try:
            self.root.mainloop()
        except Exception as e:
            self.logger.critical(f"Fatal error in GUI: {str(e)}", exc_info=True)
            # Ensure clean shutdown even on error
            if self.root and self.root.winfo_exists():
                self.root.destroy()


if __name__ == "__main__":
    try:
        # Set up basic logging before config is loaded
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        # Default config location - can be overridden with environment variable
        config_path = os.environ.get("IMMUNOLOGY_CONFIG", "config.json")
        
        # Initialize system with secure defaults
        logger.info(f"Initializing with config: {config_path}")
        immune_system = EnhancedSentientImmunology(config_path)
        
        # Start GUI securely
        gui = SecureGUI(immune_system)
        logger.info("Starting secure GUI")
        gui.run()
        
    except Exception as e:
        # Catch-all exception handler for secure shutdown
        logger = logging.getLogger(__name__)
        logger.critical(f"Fatal application error: {str(e)}", exc_info=True)
        
        # Display error to user if GUI available
        try:
            import tkinter.messagebox as messagebox
            messagebox.showerror("Fatal Error", 
                                "Application encountered a critical error and must shut down.")
        except:
            # If tkinter not available, print to console
            print(f"FATAL ERROR: {str(e)}")
